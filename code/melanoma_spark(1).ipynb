{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libs\n",
    "import findspark\n",
    "findspark.init(\"/home/antonis/spark-2.3.0-bin-hadoop2.7\")\n",
    "\n",
    "import os.path\n",
    "import pandas\n",
    "import math\n",
    "import time\n",
    "\n",
    "from metrics.Correlation import Correlation\n",
    "from IO.Write import Write\n",
    "from IO.Read import Read\n",
    "from metrics.RSquare import RSquare\n",
    "from DataSet.Dataset import DataSet\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                               OneHotEncoder , StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.driver.maxResultSize\", \"20g\")\n",
    "       .set('spark_executor_cores',\"3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark = SparkSession.builder.appName('melanoma').getOrCreate()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/home/george/diplwmatikh/code/maf/maf = 0.05/assocs/'\n",
    "#pathPatients = '/home/george/diplwmatikh/data/patients/Patients.txt'\n",
    "pathSnp = '/media/antonis/red/newSet/maf/maf = 0.05/assoc/pvalue = 0.001/snp1.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read ASSOCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "start_time = time.time()\n",
    "#path = '/home/george/snp/'\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "log_txt = sc.textFile(path)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "header = log_txt.first()\n",
    "log_txt = log_txt.filter(lambda line: line != header)\n",
    "temp_var = log_txt.map(lambda k: k.split())\n",
    "\n",
    "inferRow = temp_var.map(lambda p: Row(CHR=(p[0].strip()),SNP=p[1].strip(),BP=p[2].strip(),A1=p[3].strip(),F_A=p[4].strip()\n",
    "                                   ,F_U=p[5].strip(),A2=p[6].strip(),CHISQ=p[7].strip(),P=(p[8].strip())\n",
    "                                      ,OR=p[9].strip()))\n",
    "schemaString = \"CHR SNP BP A1 F_A F_U A2 CHISQ P OR\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "schemaAssocs = spark.createDataFrame(inferRow,schema)\n",
    "schemaAssocs.createOrReplaceTempView(\"Assocs\")\n",
    "#df=temp_var.toDF(header.split())\n",
    "#log_df.select(log_df['field2']).show()\n",
    "#log_df.filter(log_df['field2']=='1').count()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "#df =spark.read.csv(path,header=True,inferSchema=True,escape=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "#path = '/home/george/snp/'\n",
    "sc = SparkContext.getOrCreate()\n",
    "sqlContext = SQLContext(sc)\n",
    "log_txt = sc.textFile(pathPatients)\n",
    "sc.setLogLevel(\"WARN\")\n",
    "header = log_txt.first()\n",
    "log_txt = log_txt.filter(lambda line: line != header)\n",
    "temp_var = log_txt.map(lambda k: k.split())\n",
    "\n",
    "inferRow = temp_var.map(lambda p: Row(eid=(p[0].strip()),sex=p[1].strip(),birth_year=p[2].strip(),cases=p[3].strip()))\n",
    "schemaString = \"eid sex birth_year cases\"\n",
    "\n",
    "fields = [StructField(field_name, StringType(), True) for field_name in schemaString.split()]\n",
    "schema = StructType(fields)\n",
    "schemaPatients = spark.createDataFrame(inferRow,schema)\n",
    "schemaPatients.createOrReplaceTempView(\"Patients\")\n",
    "#df=temp_var.toDF(header.split())\n",
    "#log_df.select(log_df['field2']).show()\n",
    "#log_df.filter(log_df['field2']=='1').count()\n",
    "elapsed_time = time.time() - start_time\n",
    "print(elapsed_time)\n",
    "#df =spark.read.csv(path,header=True,inferSchema=True,escape=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO REMOVE NA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaAssocs = schemaAssocs.where(\"OR != 'NA'\").drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaAssocs.createOrReplaceTempView(\"Assocs\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlDF = spark.sql(\"SELECT * FROM Assocs WHERE SNP=='.' \")\n",
    "sqlDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schemaPatients.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRING INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snp_indexer = StringIndexer(inputCol='SNP',outputCol='SNPIndex')\n",
    "id_indexer = StringIndexer(inputCol='eid',outputCol='eidIndex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients =id_indexer.fit(schemaPatients).transform(schemaPatients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Patients.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assocs = snp_indexer.fit(schemaAssocs).transform(schemaAssocs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Assocs.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GET IDS FROM PATINETS AND SNP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setIdToName(aList):\n",
    "    \n",
    "    ids = {}\n",
    "    nameToId = {}\n",
    "    idToName = {}\n",
    "    count = 0\n",
    "    \n",
    "    for i in aList:\n",
    "        \n",
    "        nameToId[i] = count\n",
    "        idToName[count] = i\n",
    "        count += 1\n",
    "        \n",
    "    ids['nameToId'] = nameToId\n",
    "    ids['idToName'] = idToName\n",
    "    \n",
    "    yield ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids={}\n",
    "test= Patients.select('eid')\n",
    "#idss = test.toPandas().to_dict(orient='list')\n",
    "snps = Assocs.select('SNPIndex')\n",
    "idsa = snps.toPandas().to_dict(orient='list')\n",
    "ids['patients'] = setIdToName(test.toPandas().to_dict(orient='list')['eid'])\n",
    "ids['snps'] = idsa['SNPIndex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = read.readSnpsCode(patients,ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.toPandas().to_dict(orient='list')['eid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathSnp = '/media/antonis/red/newSet/maf/maf = 0.05/assoc/pvalue = 0.001/snp1.txt'\n",
    "pathSnp = '/media/antonis/red/newdata/maf = 0.05/pvalue = 0.001/snp2.txt'\n",
    "\n",
    "#pathSnp = '/media/antonis/Antonis_Moulopoulos/newSet/pvalue = 0.001/snp1.txt'\n",
    "#pathSnp = '/media/antonis/Antonis_Moulopoulos/newdata/maf = 0.05/pvalue = 0.001/snp2.txt'\n",
    "\n",
    "data = spark.read.csv(pathSnp,inferSchema=True,header=True)\n",
    "data=data.withColumnRenamed('TARGET','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12002\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for i in data.columns:\n",
    "    if 'rs'  in i:\n",
    "        features.append(i)\n",
    "print(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'label'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns[len(data.columns)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|patients|label|\n",
      "+--------+-----+\n",
      "|20480890| null|\n",
      "|32535401| null|\n",
      "|25233342| null|\n",
      "|50043600| null|\n",
      "|21612091| null|\n",
      "|45058371| null|\n",
      "|23464171| null|\n",
      "|53582540| null|\n",
      "|55526580| null|\n",
      "|33210700| null|\n",
      "+--------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('patients','label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|label|\n",
      "+-----+\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "| null|\n",
      "+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select('label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "assempler = VectorAssembler(inputCols=features,outputCol='features')\n",
    "output = assempler.transform(data)\n",
    "final_data = output.select('features','label')\n",
    "#final_data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORRELATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.ml.stat import Correlation\n",
    "final_data1 = final_data.select('features')\n",
    "corr = Correlation.corr(final_data1, \"features\")\n",
    "corr = corr.head()[0].toArray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  9470\n",
      "len snpsRed =  9470\n"
     ]
    }
   ],
   "source": [
    "snpsRed = []\n",
    "snpsRed = s.lowCorrelation(corr, threshold=0.7, up=100, down=99)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHI-SQUARE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from pyspark.ml.stat import ChiSquareTest\n",
    "from pyspark.ml.linalg import Vectors\n",
    "data1 = [(1.0, Vectors.dense(23.0, 5.0)),\n",
    "        (0.0, Vectors.dense(8.0, 10.0))]\n",
    "df = spark.createDataFrame(data1, [\"label\", \"features\"])\n",
    "\n",
    "r1 = ChiSquareTest.test(df, \"features\", \"label\").head()\n",
    "\n",
    "'''r = ChiSquareTest.test(final_data, \"features\", \"label\").head()'''\n",
    "print(\"pValues: \" + str(r1.pValues))\n",
    "print(\"degreesOfFreedom: \" + str(r1.degreesOfFreedom))\n",
    "print(\"statistics: \" + str(r1.statistics))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''count = 0 \n",
    "j= 0 \n",
    "for i in r[0].toArray():\n",
    "    if i >= 0.05:\n",
    "        print(features[j])\n",
    "        count += 1\n",
    "    j += 1\n",
    "print(\"count = \",count)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "snpsRed1 = []\n",
    "for i in snpsRed:\n",
    "    snpsRed1.append(features[i])\n",
    "\n",
    "assempler = VectorAssembler(inputCols=snpsRed1,outputCol='snpsRed')\n",
    "output = assempler.transform(data)\n",
    "final_data = output.select('snpsRed','label')\n",
    "#final_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9470"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(snpsRed1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data,test_data = final_data.randomSplit([0.9,0.1],seed=11)\n",
    "\n",
    "train_data=train_data.withColumnRenamed('TARGET','label')\n",
    "test_data=test_data.withColumnRenamed('TARGET','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final =  242176\n",
      "test_data =  24214\n",
      "train_data =  217962\n",
      "all =  242176\n"
     ]
    }
   ],
   "source": [
    "print(\"final = \",final_data.count())\n",
    "print(\"test_data = \", test_data.count())\n",
    "print(\"train_data = \", train_data.count())\n",
    "print(\"all = \", test_data.count() + train_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='snpsRed',labelCol='label',maxIter=10)\n",
    "lr_model = log_reg.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = lr_model.transform(test_data)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluate = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='label')\n",
    "AUC = evaluate.evaluate(results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Param for metric name in evaluation. Supports: - \"rmse\" (default): root mean squared error - \n",
    "\"mse\": mean squared error - \"r2\": R^2^ metric - \"mae\": mean absolute error '''\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#lr = LinearRegression(maxIter=maxIteration)\n",
    "#pipeline = Pipeline(stages=[output, log_reg])\n",
    "pipeline = Pipeline(stages=[log_reg])\n",
    "#fit_model = pipeline.fit(train_data)\n",
    "modelEvaluator=RegressionEvaluator(predictionCol='prediction', labelCol='label')\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval = CrossValidator(estimator=log_reg,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=modelEvaluator,\n",
    "                          numFolds=10)\n",
    "\n",
    "cvModel = crossval.fit(train_data)\n",
    "cross_results = cvModel.transform(test_data)\n",
    "results = cross_results.select('prediction','label')\n",
    "#results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[snpsRed: vector, label: int, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseMatrix(2, 2, [23988.0, 221.0, 4.0, 1.0], 0)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "c = cross_results.rdd\n",
    "predictionAndLabels = c.map(lambda lp: (lp.prediction, float(lp.label)))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)\n",
    "metrics.confusionMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.3988e+04, 4.0000e+00],\n",
       "       [2.2100e+02, 1.0000e+00]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.9907078549599405\n",
      "recal =  0.0045045045045045045\n",
      "precision =  0.2\n",
      "f1 =  0.9953320470529656\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy = \", metrics.accuracy)\n",
    "print(\"recal = \", metrics.recall(2))\n",
    "print(\"precision = \", metrics.precision(2))\n",
    "print(\"f1 = \", metrics.fMeasure(1.0))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9836202592506461"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.weightedPrecision\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9862873717696917"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.weightedFMeasure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = r.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "t= r1.freqItems(['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "r3 = r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "same1 = 0\n",
    "same2 = 0\n",
    "sum1 = 0\n",
    "sum2 = 0\n",
    "sumall = 0\n",
    "for i in r3:\n",
    "    \n",
    "    if i[1] == 1:\n",
    "        sum1 += 1\n",
    "        if i[0] == 1:\n",
    "            same1 += 1\n",
    "    elif i[1] == 2:\n",
    "        sum2 += 1\n",
    "        if i[0] == 2:\n",
    "            same2 += 1\n",
    "    sumall += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum1 =  23992\n",
      "sum2 =  222\n",
      "same1 =  23988\n",
      "same2 =  1\n",
      "all =  24214\n",
      "all2 =  24214\n"
     ]
    }
   ],
   "source": [
    "print('sum1 = ', sum1)\n",
    "print('sum2 = ', sum2)\n",
    "print('same1 = ', same1)\n",
    "print('same2 = ', same2)\n",
    "print('all = ', sumall)\n",
    "print('all2 = ', sum1+sum2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
