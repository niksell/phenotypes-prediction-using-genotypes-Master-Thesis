{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import libs\n",
    "import findspark\n",
    "findspark.init(\"/home/antonis/spark-2.3.0-bin-hadoop2.7\")\n",
    "\n",
    "import os.path\n",
    "import pandas\n",
    "import math\n",
    "import time\n",
    "\n",
    "from metrics.Correlation import Correlation\n",
    "from IO.Write import Write\n",
    "from IO.Read import Read\n",
    "from metrics.RSquare import RSquare\n",
    "from DataSet.Dataset import DataSet\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "import sys\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import countDistinct,avg,stddev\n",
    "from pyspark.ml.feature import (VectorAssembler,VectorIndexer,\n",
    "                               OneHotEncoder , StringIndexer)\n",
    "import DataSet.SnpsSelection as s\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "conf = (SparkConf()\n",
    "    .set(\"spark.driver.maxResultSize\", \"20g\")\n",
    "       .set('spark_executor_cores',\"3\"))\n",
    "\n",
    "#spark = SparkSession.builder.appName('melanoma').getOrCreate()\n",
    "sc = SparkContext(conf=conf)\n",
    "spark = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathSnp = '/media/antonis/red/newSet/maf/maf = 0.05/assoc/pvalue = 0.001/snp1.txt'\n",
    "#pathSnp = '/media/antonis/red/newSet/maf/maf = 0.05/assoc/pvalue = 0.01/snp2.txt'\n",
    "pathSnp = '/media/antonis/red/newdata/maf = 0.05/pvalue = 0.001/snp2.txt'\n",
    "\n",
    "#pathSnp = '/media/antonis/Antonis_Moulopoulos/newSet/pvalue = 0.001/snp1.txt'\n",
    "#pathSnp = '/media/antonis/Antonis_Moulopoulos/newdata/maf = 0.05/pvalue = 0.001/snp2.txt'\n",
    "\n",
    "data = spark.read.option(\"maxColumns\", 80000).csv(pathSnp,inferSchema=True,header=True)\n",
    "data=data.withColumnRenamed('TARGET','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    }
   ],
   "source": [
    "features = []\n",
    "for i in data.columns:\n",
    "    if 'rs' not in i and i !='label':\n",
    "        features.append(i)\n",
    "print(len(features))\n",
    "#print((features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.select('patients','label').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "columns =  12003\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.linalg import DenseVector\n",
    "\n",
    "d = data.drop('patients')\n",
    "for i in features:\n",
    "    d = d.drop(i)\n",
    "print(\"columns = \",len(d.columns))\n",
    "li = len(d.columns)\n",
    "input_data = d.rdd.map(lambda x: (x[li-1], DenseVector(x[:li-1])))\n",
    "\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,2.0,1.0,...|\n",
      "|    0|[0.0,0.0,1.0,1.0,...|\n",
      "|    1|[0.0,1.0,1.0,0.0,...|\n",
      "|    0|[1.0,0.0,1.0,0.0,...|\n",
      "|    0|[0.0,1.0,1.0,0.0,...|\n",
      "|    0|[1.0,0.0,1.0,0.0,...|\n",
      "|    0|[1.0,1.0,1.0,0.0,...|\n",
      "|    0|[1.0,1.0,1.0,1.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.stat import Correlation\n",
    "final_data1 = df.select('features')\n",
    "corr = Correlation.corr(final_data1, \"features\")\n",
    "corr = corr.head()[0].toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count =  9470\n",
      "len snpsRed =  9470\n"
     ]
    }
   ],
   "source": [
    "snpsRed = []\n",
    "snpsRed = s.lowCorrelation(corr, threshold=0.7, up=100, down=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12002\n",
      "len features1 =  12002\n",
      "len features2 =  2673\n",
      "columns =  9471\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d = data.drop('patients')\n",
    "features = []\n",
    "\n",
    "for i in data.columns:\n",
    "    if 'rs' in i and i !='label':\n",
    "        features.append(i)\n",
    "print(len(features))\n",
    "\n",
    "#features = d.columns\n",
    "\n",
    "print(\"len features1 = \",len(features))\n",
    "\n",
    "snpsRed1 = []\n",
    "for i in snpsRed:\n",
    "    snpsRed1.append(features[i])\n",
    "\n",
    "snpsRed1.append('label')\n",
    "    \n",
    "features = []\n",
    "for i in data.columns:\n",
    "    if i not in snpsRed1:\n",
    "        features.append(i)\n",
    "print(\"len features2 = \",len(features))\n",
    "\n",
    "for i in features:\n",
    "    d = d.drop(i)\n",
    "print(\"columns = \",len(d.columns))\n",
    "li = len(d.columns)\n",
    "input_data = d.rdd.map(lambda x: (x[li-1], DenseVector(x[:li-1])))\n",
    "\n",
    "df = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "#assempler = VectorAssembler(inputCols=snpsRed1,outputCol='snpsRed')\n",
    "#output = assempler.transform(data)\n",
    "#final_data = output.select('snpsRed','label')\n",
    "#final_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_data,test_data = final_data.randomSplit([0.9,0.1],seed=11)\n",
    "\n",
    "train_data,test_data = df.randomSplit([0.9,0.1],seed=11)\n",
    "\n",
    "train_data=train_data.withColumnRenamed('TARGET','label')\n",
    "test_data=test_data.withColumnRenamed('TARGET','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0,t2 = train_data.filter('label == 0').randomSplit([0.001,0.1],seed = 11)\n",
    "train1 = train_data.filter(\"label == 1\")\n",
    "#t0 = train0.count()\n",
    "#t1 = train1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = train0.union(train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(featuresCol='features',labelCol='label',maxIter=10)\n",
    "lr_model = log_reg.fit(train0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"t0 = \",t0)\n",
    "#print(\"t1 = \",t1)\n",
    "#print(\"train0\", train0.count())\n",
    "#print(\"all =\", t0+t1)\n",
    "\n",
    "#print(\"final = \",df.count())\n",
    "#print(\"test_data = \", test_data.count())\n",
    "#print(\"train_data = \", train_data.count())\n",
    "#print(\"all = \", test_data.count() + train_data.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8249659619542328"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = lr_model.transform(test_data)\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "evaluate = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='label')\n",
    "AUC = evaluate.evaluate(results)\n",
    "AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[0.44220094195598...|[0.60878334601552...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[5.05137818050948...|[0.99364019963049...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[2.56100442277661...|[0.92830933195703...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[5.86288126257737...|[0.99716501963883...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[-0.7378737329609...|[0.32346927469249...|       1.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[3.60328917335768...|[0.97348802931815...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[-0.8540668432135...|[0.29858044082104...|       1.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[10.6547725584097...|[0.99997641255894...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[4.25464560894296...|[0.98600064303036...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[1.82517511906496...|[0.86118594233556...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Param for metric name in evaluation. Supports: - \"rmse\" (default): root mean squared error - \n",
    "\"mse\": mean squared error - \"r2\": R^2^ metric - \"mae\": mean absolute error '''\n",
    "\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "\n",
    "#lr = LinearRegression(maxIter=maxIteration)\n",
    "#pipeline = Pipeline(stages=[output, log_reg])\n",
    "pipeline = Pipeline(stages=[log_reg])\n",
    "#fit_model = pipeline.fit(train_data)\n",
    "modelEvaluator=RegressionEvaluator(predictionCol='prediction', labelCol='label')\n",
    "paramGrid = ParamGridBuilder().build()\n",
    "\n",
    "crossval = CrossValidator(estimator=log_reg,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=modelEvaluator,\n",
    "                          numFolds=4)\n",
    "\n",
    "cvModel = crossval.fit(train0)\n",
    "cross_results = cvModel.transform(test_data)\n",
    "results = cross_results.select('prediction','label')\n",
    "#results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[label: bigint, features: vector, rawPrediction: vector, probability: vector, prediction: double]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "\n",
    "c = cross_results.rdd\n",
    "predictionAndLabels = c.map(lambda lp: (lp.prediction, float(lp.label)))\n",
    "metrics = MulticlassMetrics(predictionAndLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20089.,  3892.],\n",
       "       [   43.,   186.]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.confusionMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  0.8374638579099546\n",
      "recal =  0.9978640969600636\n",
      "precision =  0.8377048496726576\n",
      "f1 =  0.08637102391455768\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy = \", metrics.accuracy)\n",
    "print(\"recal = \", metrics.recall(0))\n",
    "print(\"precision = \", metrics.precision(0))\n",
    "print(\"f1 = \", metrics.fMeasure(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = results.rdd\n",
    "r3 = r.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "same0 = 0\n",
    "same1 = 0\n",
    "sum0 = 0\n",
    "sum1 = 0\n",
    "sumall = 0\n",
    "for i in r3:\n",
    "    \n",
    "    if i[1] == 0:\n",
    "        sum0 += 1\n",
    "        if i[0] == 0:\n",
    "            same0 += 1\n",
    "    elif i[1] == 1:\n",
    "        sum1 += 1\n",
    "        if i[0] == 1:\n",
    "            same1 += 1\n",
    "    sumall += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum0 =  23981\n",
      "sum1 =  229\n",
      "same0 =  20089\n",
      "same1 =  186\n",
      "all =  24210\n",
      "all2 =  24210\n"
     ]
    }
   ],
   "source": [
    "print('sum0 = ', sum0)\n",
    "print('sum1 = ', sum1)\n",
    "print('same0 = ', same0)\n",
    "print('same1 = ', same1)\n",
    "print('all = ', sumall)\n",
    "print('all2 = ', sum0+sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "229"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.filter(\"label==1\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[0.44220094195599...|[0.60878334601552...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[5.05137818050953...|[0.99364019963049...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[2.56100442277666...|[0.92830933195704...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[5.86288126257740...|[0.99716501963883...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[-0.7378737329609...|[0.32346927469249...|       1.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[3.60328917335766...|[0.97348802931815...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[-0.8540668432135...|[0.29858044082104...|       1.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[10.6547725584098...|[0.99997641255894...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[4.25464560894294...|[0.98600064303036...|       0.0|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|[1.82517511906497...|[0.86118594233556...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_results.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Coefficients: \\n\" + str(lr_model.coefficientMatrix.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9470"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lr_model.coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03604068, -0.0529003 , -0.01648724, ..., -0.00118198,\n",
       "         0.04564904, -0.0080924 ]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_model.coefficientMatrix.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9471\n",
      "label\n"
     ]
    }
   ],
   "source": [
    "print(len(d.columns))\n",
    "print(d.columns[len(d.columns)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.03604068230256025\n"
     ]
    }
   ],
   "source": [
    "for i in lr_model.coefficientMatrix.toArray()[0]:\n",
    "    print(i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
