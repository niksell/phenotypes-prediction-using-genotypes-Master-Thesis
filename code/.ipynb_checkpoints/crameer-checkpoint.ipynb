{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0a07a08e616>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "\n",
    "import os\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "#import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os.path\n",
    "import csv\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import grid_search\n",
    "import sklearn.linear_model as linear_model\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn import preprocessing\n",
    "#import statsmodels.api as sm\n",
    "#import statsmodels.formula.api as smf\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from random import randint\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import math\n",
    "import time\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "\n",
    "from metrics.Correlation import Correlation\n",
    "from IO.Output import Output\n",
    "from IO.Input import Input\n",
    "from metrics.RSquare import RSquare\n",
    "from DataSet.Dataset import DataSet\n",
    "\n",
    "\n",
    "def setIdToName(aList):\n",
    "    \n",
    "    ids = {}\n",
    "    nameToId = {}\n",
    "    idToName = {}\n",
    "    count = 0\n",
    "    \n",
    "    for i in aList:\n",
    "        \n",
    "        nameToId[i] = count\n",
    "        idToName[count] = i\n",
    "        count += 1\n",
    "        \n",
    "    ids['nameToId'] = nameToId\n",
    "    ids['idToName'] = idToName\n",
    "    \n",
    "    return ids\n",
    "\n",
    "def cramers_v(confusion_matrix):\n",
    "    \"\"\" calculate Cramers V statistic for categorial-categorial association.\n",
    "        uses correction from Bergsma and Wicher,\n",
    "        Journal of the Korean Statistical Society 42 (2013): 323-328\n",
    "    \"\"\"\n",
    "    print(\"ok1\")\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    \n",
    "    \n",
    "    n = confusion_matrix.sum()\n",
    "    print(\"n  = \",n)\n",
    "    phi2 = chi2 / n\n",
    "    r, k = confusion_matrix.shape\n",
    "    phi2corr = max(0, phi2 - ((k-1)*(r-1))/(n-1))\n",
    "    rcorr = r - ((r-1)**2)/(n-1)\n",
    "    kcorr = k - ((k-1)**2)/(n-1)\n",
    "    \n",
    "    print(\"val = \",phi2corr)\n",
    "    \n",
    "    return np.sqrt(phi2corr / min((kcorr-1), (rcorr-1)))\n",
    "\n",
    "\n",
    "def cramers_stat(confusion_matrix):\n",
    "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
    "    n = confusion_matrix.sum()\n",
    "    return np.sqrt(chi2 / (n*(min(confusion_matrix.shape)-1)))\n",
    "\n",
    "\n",
    "def createCrammerTable(X):\n",
    "    \n",
    "    cram = np.zeros((len(X.T),len(X.T)), dtype = np.float32)\n",
    "    \n",
    "    y = np.zeros((2,len(X)), dtype = np.int32)\n",
    "    \n",
    "    for i in range(len(X.T)):\n",
    "        for j in range(i,len(X.T)):\n",
    "            \n",
    "            y[0,:] = X[:,i]\n",
    "            y[1,:] = X[:,j]\n",
    "            \n",
    "            v = cramers_stat(y)\n",
    "            \n",
    "            cram[i,j] = v\n",
    "            cram[j,i] = v\n",
    "            \n",
    "    return cram\n",
    "\n",
    "\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "################################################################################################################################\n",
    "\n",
    "def tables(sampleX,sampleY,k):\n",
    "  \n",
    "    samples = {}\n",
    "    \n",
    "    for run in range(1,k+1):\n",
    "        \n",
    "        d1 = {}\n",
    "        \n",
    "\n",
    "        dataTestX = sampleX[run]\n",
    "        dataTestY = sampleY[run]\n",
    "\n",
    "        n = 0\n",
    "\n",
    "        for i in sampleX.keys():\n",
    "\n",
    "            if i != run:\n",
    "\n",
    "                n += len(sampleX[i])\n",
    "\n",
    "        dataTrainX = np.zeros((n,len(sampleX[1].T)),dtype = int)\n",
    "        dataTrainY = np.zeros((n,),dtype = int)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for sample in sampleX.keys():\n",
    "\n",
    "            if sample != run:\n",
    "\n",
    "                 for i in range(len(sampleX[sample])):\n",
    "                    for j in range(len(sampleX[sample].T)):\n",
    "                        dataTrainX[count,j] = sampleX[sample][i,j]\n",
    "\n",
    "                    dataTrainY[count] = sampleY[sample][i]\n",
    "                    count += 1\n",
    "\n",
    "        d1['trainX'] = dataTrainX\n",
    "        d1['trainY'] = dataTrainY\n",
    "        d1['testX'] = dataTestX\n",
    "        d1['testY'] = dataTestY\n",
    "        \n",
    "        samples[run] = d1\n",
    "    \n",
    "    return samples\n",
    "    \n",
    "def kSampleData(k,X,Y):\n",
    "    \n",
    "    x = int (len(X) / k)\n",
    "    allElements = np.zeros((len(X),),dtype = int)\n",
    "    \n",
    "    count1 = 1\n",
    "    sampleX = {}\n",
    "    sampleY = {}\n",
    "   \n",
    "    \n",
    "    while count1 <= k:\n",
    "        count2 = 1\n",
    "        sampleData = []\n",
    "        \n",
    "        if count1 == k:\n",
    "            x =  len(X) - ((k-1) * x)\n",
    "        \n",
    "        dataX = np.zeros((x,len(X.T)),dtype = int)\n",
    "        dataY = np.zeros((x,),dtype = int)\n",
    "        \n",
    "        while count2 <= x:\n",
    "            \n",
    "            aRand = randint(0,len(X)-1)\n",
    "            \n",
    "            while allElements[aRand] == 1:\n",
    "                \n",
    "                aRand = randint(0,len(X)-1)\n",
    "            \n",
    "            allElements[aRand] = 1\n",
    "            sampleData.append(aRand)\n",
    "            count2 += 1\n",
    "            \n",
    "        for i in range(len(sampleData)):\n",
    "            for j in range(len(X.T)):\n",
    "                dataX[i,j] = X[sampleData[i],j]\n",
    "            \n",
    "            dataY[i] = Y[sampleData[i]]\n",
    "            \n",
    "        sampleX[count1] = dataX\n",
    "        sampleY[count1] = dataY\n",
    "        count1 +=1\n",
    "        \n",
    "    return tables(sampleX,sampleY,k)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def createNewTable(snps,X):\n",
    "    \n",
    "    newX = np.zeros((len(X),len(snps)),dtype = np.int32)\n",
    "    count=0\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(len(newX)):\n",
    "        for j in range(len(newX.T)):\n",
    "            newX[i,j] = -1\n",
    "    \n",
    "    for i in range(len(snps)):\n",
    "        \n",
    "        newX[:,i] = X[:,snps[i]]\n",
    "        \n",
    "        \n",
    "    print(\"new shape = \",newX.shape)\n",
    "            \n",
    "    return newX \n",
    "\n",
    "\n",
    "\n",
    "def featuresIds(oldSnps,snps):\n",
    "    \n",
    "    features = {}\n",
    "    \n",
    "    for i in range(len(snps)):\n",
    "        features[i] = snps[i]\n",
    "        \n",
    "    return features\n",
    "\n",
    "\n",
    "\n",
    "def crossValidiation(X, Y, k = 1, continious = True, classifier = None,OLS = False,Logistic = False):\n",
    "    \n",
    "    if not classifier:\n",
    "        print(\"wrong!!!!!!! you have to choise a classifier\")\n",
    "        return\n",
    "    \n",
    "    results = {}\n",
    "    accuracy = {}\n",
    "    auc = {}\n",
    "    recall = {}\n",
    "    precision = {}\n",
    "    f1Score = {}\n",
    "    \n",
    "    sumResults = 0.0\n",
    "    sumAccuracy = 0.0\n",
    "    sumAuc = 0.0\n",
    "    sumRecall = 0.0\n",
    "    sumPrecision = 0.0\n",
    "    sumF1Score = 0.0\n",
    "    \n",
    "    samples = kSampleData(k,X,Y)\n",
    "    \n",
    "    for run in range(1, k + 1):\n",
    "        \n",
    "        trainX = samples[run]['trainX']\n",
    "        trainY = samples[run]['trainY']\n",
    "        \n",
    "        #trainX,trainY = balancedData(trainX,trainY)\n",
    "        \n",
    "        testX = samples[run]['testX']\n",
    "        testY = samples[run]['testY']\n",
    "        \n",
    "        \n",
    "        if OLS:\n",
    "            classifier = sm.OLS(trainY,trainX)\n",
    "            yPredict = classifier.fit().predict(testX)\n",
    "        else:\n",
    "\n",
    "            classifier.fit(trainX, trainY)\n",
    "            yPredict = classifier.predict(testX)\n",
    "        \n",
    "        if continious:\n",
    "            \n",
    "            for i in range(len(yPredict)):\n",
    "                \n",
    "                if (abs(0 - yPredict[i]) - abs(1 - yPredict[i])) <= 1e-10 :\n",
    "                    yPredict[i] = 0\n",
    "                else:\n",
    "                    yPredict[i] = 1\n",
    "                    \n",
    "        if Logistic:\n",
    "            \n",
    "            probabilities = classifier.predict_proba(testX)\n",
    "            \n",
    "            for i in range(len(probabilities)):\n",
    "                if probabilities[i][1] >= 0.8:\n",
    "                    yPredict[i] = 1\n",
    "                else:\n",
    "                    yPredict[i] = 0\n",
    "          \n",
    "        accuracy[run] = metrics.accuracy_score(testY,yPredict)#(yPredict,testY)#\n",
    "        fpr, tpr, thresholds = metrics.roc_curve(testY,yPredict)\n",
    "        auc[run] = metrics.auc(fpr,tpr)\n",
    "        recall[run] = metrics.recall_score(testY,yPredict)\n",
    "        precision[run] = metrics.precision_score(testY,yPredict)\n",
    "        f1Score[run] = f1_score(testY, yPredict, average='binary')\n",
    "        \n",
    "    \n",
    "    for i in accuracy.keys():\n",
    "        sumAccuracy = sumAccuracy + accuracy[i]\n",
    "        sumAuc = sumAuc + auc[i]\n",
    "        sumRecall = sumRecall + recall[i]\n",
    "        sumPrecision = sumPrecision + precision[i]\n",
    "        sumF1Score = sumF1Score + f1Score[i]\n",
    "    \n",
    "    results['accuracy'] = sumAccuracy / k\n",
    "    results['auc'] = sumAuc / k\n",
    "    results['recall'] = sumRecall / k\n",
    "    results['precision'] = sumPrecision / k\n",
    "    results['f1'] = sumF1Score / k\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def writeCoef(path,snpsIds,sc,idToName,name = None):\n",
    "        \n",
    "        if not name:\n",
    "            print(\"give a name to file\")\n",
    "            return\n",
    "        \n",
    "        p = path + name  + \" ( \" + time.strftime(\"%d-%m-%Y\") + \" ).txt \"  \n",
    "    \n",
    "        i=1\n",
    "        while os.path.exists(p):\n",
    "            \n",
    "            p = path + name  + \" ( \" + time.strftime(\"%d-%m-%Y\") + \" ) \" + '_' + str(i)+\".txt\"\n",
    "            i += 1\n",
    "        \n",
    "        snps = []\n",
    "        for i in range(len(snpsIds)):\n",
    "            s = snpsIds[i]\n",
    "            snps.append(idToName[s])\n",
    "            \n",
    "        print(\"snpsIds = \",len(snpsIds))\n",
    "        print(\"idToName = \",len(idToName))\n",
    "        \n",
    "        write = open(p,'w')\n",
    "        for i in range(len(snps)):\n",
    "            \n",
    "            write.write(str(snps[i])+'\\t'+str(sc[i])+'\\n')\n",
    "            \n",
    "        write.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'C:\\\\Users\\\\ANTONIS\\\\Desktop\\\\newSet\\\\maf\\\\maf = 0.05\\\\assoc\\\\pvalue = 0.001\\\\'\n",
    "#path = 'C:\\\\Users\\\\ANTONIS\\\\Desktop\\\\newSet\\\\maf\\\\maf = 0.05\\\\assoc\\\\pvalue = 0.01\\\\'\n",
    "#path = 'D:\\\\newSet\\\\pvalue = 0.001\\\\'\n",
    "#path = 'D:\\\\newdata\\\\maf = 0.05\\\\pvalue = 0.001\\\\'\n",
    "\n",
    "path = '/media/antonis/red/newSet/maf/maf = 0.05/assoc/pvalue = 0.001/'\n",
    "\n",
    "#path = '/media/antonis/red/newdata/maf = 0.05/pvalue = 0.001/'\n",
    "#path = '/media/antonis/red/newSet/maf/maf = 0.05/assoc/pvalue = 0.01/'\n",
    "\n",
    "numberOfChromosomes = 22#'ari8mos twn xromoswmatwn'\n",
    "chromosomes = {}\n",
    "\n",
    "read = Input(path,numberOfChromosomes)\n",
    "write = Output(path,numberOfChromosomes)\n",
    "\n",
    "patients = read.readPatients('Patients.txt')\n",
    "chromosomes = read.readSnps(\".assoc\")\n",
    "write.writePatientsList(patients,'patient.txt')\n",
    "write.writeSnpsList(chromosomes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snps = read.getListOfSnps()\n",
    "ids = {} \n",
    "\n",
    "\n",
    "ids['patients'] = setIdToName(list(patients.keys()))\n",
    "ids['snps'] = setIdToName(snps)\n",
    "\n",
    "\n",
    "if os.path.exists(path + 'snpCode.txt'):\n",
    "    print(\"mphka\")\n",
    "    X, Y = read.readSnpsCode(patients,ids)\n",
    "    \n",
    "else:\n",
    "    \n",
    "    write.writeSnpLog(read.getNumberOfPatients(),read.getNumberOfSnps(),chromosomes,read.getSnpsList(),patients)\n",
    "    #X, Y = read.readSnpsCode(patients,ids)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''for i in range(len(X)):\n",
    "    for j in range(len(X.T)):\n",
    "        \n",
    "        X[i,j] = X[i,j] + 1'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum1 =0\n",
    "sum0 =0\n",
    "for i in range(len(Y)):\n",
    "    \n",
    "    if Y[i] == 0:\n",
    "        sum0 += 1\n",
    "        \n",
    "    elif Y[i] == 1:\n",
    "        sum1+=1\n",
    "        \n",
    "print(\"sum0 = \",sum0)\n",
    "print(\"sum1 \", sum1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''sum0 =0\n",
    "sum1 =0\n",
    "sum2 =0\n",
    "sum3 =0\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X.T)):\n",
    "        \n",
    "        if X[i,j] == 0:\n",
    "            sum0 += 1\n",
    "        elif X[i,j] == 1:\n",
    "            sum1 += 1\n",
    "        elif X[i,j] == 2:\n",
    "            sum2 += 1\n",
    "        elif X[i,j] == 3:\n",
    "            sum3 += 1\n",
    "            \n",
    "print(\"sum0 = \", sum0)\n",
    "print(\"sum1 = \", sum1)\n",
    "print(\"sum2 = \", sum2)\n",
    "print(\"sum3 = \", sum3)'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xTraining, xTest, yTraining1, yTest = train_test_split(X, Y, test_size=0.1, random_state=randint(0,2018))\n",
    "print(\"mergex = \",X.shape)\n",
    "print(\"xTrain = \",xTraining.shape)\n",
    "print(\"xTest = \",xTest.shape)\n",
    "print(\"yTrain = \",yTraining1.shape)\n",
    "print(\"yTest = \",yTest.shape)\n",
    "\n",
    "cor = Correlation(X)\n",
    "snpReduc = {}\n",
    "\n",
    "down = 100\n",
    "up = 100 \n",
    "threshold = 0.7\n",
    "snpReduc['low100'] = cor.getLowCorrelationSnps(0.7, down=100,up=100,c=-2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cram = createCrammerTable(X)\n",
    "print(\"shape = \", cram.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _counterSnps(n):\n",
    "    snpsCount = {}\n",
    "    for i in range(n):\n",
    "        snpsCount[i] = 0\n",
    "    \n",
    "    return snpsCount\n",
    "\n",
    "\n",
    "def _highClass(X,b,c):\n",
    "\n",
    "    snpsCount = _counterSnps(len(X.T))\n",
    "\n",
    "    for i in range(len(X)):\n",
    "\n",
    "        for j in range(i+1,len(X.T)):\n",
    "\n",
    "            if X[i,j] - b > 1e-10 and X[i,j] >= c:\n",
    "\n",
    "                snpsCount[i] = snpsCount[i] + 1\n",
    "                snpsCount[j] = snpsCount[j] + 1\n",
    "\n",
    "    return snpsCount\n",
    "\n",
    "\n",
    "def highCorrelation(X, b,up,down, c=-2):\n",
    "\n",
    "    snpsRed = []\n",
    "    count = 0\n",
    "    \n",
    "    snpsCount = _highClass(X,b,c)\n",
    "\n",
    "    for i in snpsCount.keys():\n",
    "        if snpsCount[i] >= down * (len(X.T)-1) / 100 and snpsCount[i] <= up * (len(X.T)-1) / 100:\n",
    "\n",
    "            snpsRed.append(i)\n",
    "            count += 1\n",
    "\n",
    "    #print(\"count = \",count)\n",
    "    #print(\"len snpsRed = \",len(snpsRed))\n",
    "    \n",
    "    return snpsRed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''maxx = cram[0,0]\n",
    "\n",
    "for i in range(len(X)):\n",
    "    for j in range(len(X.T)):\n",
    "        \n",
    "        if maxx <= cram[i,j]:\n",
    "            maxx = cram[i,j]\n",
    "        \n",
    "print('max = ', maxx)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snpReduc['cram'] = highCorrelation(cram,0.1,100,100)\n",
    "print(\"len red = \",len(snpReduc['cram']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "#categories = ['low','high']\n",
    "#categories = ['low']\n",
    "#categories = ['low97','low100']\n",
    "categories = ['low100','cram']\n",
    "writeResults2={}\n",
    "writeResults={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import NuSVC\n",
    "\n",
    "for i in categories:\n",
    "    \n",
    "    r1 = {}\n",
    "    r1['down'] = down\n",
    "    r1['up'] = up\n",
    "    r1['thres'] = threshold\n",
    "    writeResults={}\n",
    "    print(\"Category = \",i)\n",
    "    \n",
    "    XX = createNewTable(snpReduc[i],X)\n",
    "    xTraining1 = createNewTable(snpReduc[i],xTraining)\n",
    "    xTest1 = createNewTable(snpReduc[i],xTest)\n",
    "    \n",
    "    \n",
    "    \n",
    " \n",
    "    \n",
    "    \n",
    "    clf = SVC(kernel='linear')\n",
    "    #clf = NuSVC(kernel='rbf',nu=0.01)\n",
    "    clf.fit(xTraining1, yTraining1)\n",
    "    yPredict2 = clf.predict(xTest1)\n",
    "    print(metrics.accuracy_score(yTest,yPredict2))\n",
    "    print(metrics.confusion_matrix(yTest,yPredict2))\n",
    "    error2 = mean_squared_error(yTest, yPredict2)\n",
    "    print(\"error 2 = \",error2)\n",
    "    RMSE2 = mean_squared_error(yTest,yPredict2)**0.5\n",
    "    print(\"RMSE2 = \",RMSE2)\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(yTest,yPredict2)\n",
    "    print(\"AUC = \", metrics.auc(fpr,tpr))\n",
    "    print(\"recal = \",metrics.recall_score(yTest,yPredict2))\n",
    "    print(\"precision = \",metrics.precision_score(yTest,yPredict2))\n",
    "    print(\"f1Score = \",f1_score(yTest, yPredict2, average='binary'))\n",
    "    \n",
    "    r1['confu'] = metrics.confusion_matrix(yTest,yPredict2)\n",
    "    r1['accu'] = metrics.accuracy_score(yTest,yPredict2)\n",
    "    r1['auc'] = metrics.auc(fpr,tpr)\n",
    "    r1['recal'] = metrics.recall_score(yTest,yPredict2)\n",
    "    r1['precision'] = metrics.precision_score(yTest,yPredict2)\n",
    "    r1['f1'] = f1_score(yTest, yPredict2, average='binary')\n",
    "    \n",
    "    writeResults2[i] = r1\n",
    "    print()\n",
    "\n",
    "    results = crossValidiation(XX, Y, k = 10, classifier = clf, continious = False)\n",
    "    results['down'] = down\n",
    "    results['up'] = up\n",
    "    results['thres'] = threshold\n",
    "    print(\"accuracy = \",results['accuracy'])\n",
    "    print(\"AUC = \", results['auc'])\n",
    "    print(\"recal = \",results['recall'])\n",
    "    print(\"precision = \",results['precision'])\n",
    "    print(\"f1 = \",results['f1'])\n",
    "    results['len_snps'] = len(snpReduc[i])\n",
    "   # results['len snps'] = 500\n",
    "    writeResults[i] = results\n",
    "    \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in categories:\n",
    "    \n",
    "    r1 = {}\n",
    "    writeResults={}\n",
    "    print(\"Category = \",i)\n",
    "    \n",
    "   \n",
    "    XX = createNewTable(snpReduc[i],X)\n",
    "    xTraining1 = createNewTable(snpReduc[i],xTraining)\n",
    "    xTest1 = createNewTable(snpReduc[i],xTest)\n",
    "    \n",
    "    \n",
    "    \n",
    "    '''test = SelectKBest(score_func=chi2,k = 500)\n",
    "    fit1 = test.fit(xTraining1, yTraining1)\n",
    "    xTraining1 = fit1.transform(xTraining1)\n",
    "\n",
    "    fit2 = test.fit(xTest1, yTest)\n",
    "    xTest1 = fit2.transform(xTest1)\n",
    "\n",
    "    fit3 = test.fit(XX, Y)\n",
    "    XX = fit3.transform(XX)\n",
    "\n",
    "    print(\"xtest\", xTest1.shape)\n",
    "    print(\"xtraining\", xTraining1.shape)\n",
    "    print(\"X\", XX.shape)'''\n",
    "   \n",
    "    '''enc = OneHotEncoder(n_values =3) \n",
    "    enc.fit(XX) \n",
    "    XX = enc.fit_transform(XX)\n",
    "    XX = XX.toarray()\n",
    "    print(XX.shape)\n",
    "    enc.fit(xTraining1) \n",
    "    xTraining1 = enc.fit_transform(xTraining1)\n",
    "    xTraining1 = xTraining1.toarray()\n",
    "    print(xTraining1.shape)\n",
    "\n",
    "    enc.fit(xTest1) \n",
    "    xTest1 = enc.fit_transform(xTest1)\n",
    "    xTest1 = xTest1.toarray()\n",
    "    print(xTest1.shape)\n",
    "    '''\n",
    "     \n",
    "\n",
    "    #lr_clf = linear_model.LogisticRegression()\n",
    " \n",
    "    lr_clf = linear_model.LogisticRegressionCV()\n",
    "    lr_clf.fit(xTraining1, yTraining1)\n",
    "    \n",
    "    yPredict4 = lr_clf.predict(xTest1)\n",
    "\n",
    "    print(metrics.accuracy_score(yTest,yPredict4))\n",
    "    print(metrics.confusion_matrix(yTest,yPredict4))\n",
    "    error4 = mean_squared_error(yTest, yPredict4)\n",
    "    print(\"error 4 = \",error4)\n",
    "    RMSE4 = mean_squared_error(yTest,yPredict4)**0.5\n",
    "    print(\"RMSE4 = \",RMSE4)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(yTest,yPredict4)\n",
    "    print(\"AUC = \", metrics.auc(fpr,tpr))\n",
    "    print(\"recal = \",metrics.recall_score(yTest,yPredict4))\n",
    "    print(\"precision = \",metrics.precision_score(yTest,yPredict4))\n",
    "    print(\"f1Score = \",f1_score(yTest, yPredict4, average='binary'))\n",
    "    \n",
    "    r1['confu'] = metrics.confusion_matrix(yTest,yPredict4)\n",
    "    r1['accu'] = metrics.accuracy_score(yTest,yPredict4)\n",
    "    r1['auc'] = metrics.auc(fpr,tpr)\n",
    "    r1['recal'] = metrics.recall_score(yTest,yPredict4)\n",
    "    r1['precision'] = metrics.precision_score(yTest,yPredict4)\n",
    "    r1['f1'] = f1_score(yTest, yPredict4, average='binary')\n",
    "    writeResults2[i] = r1\n",
    "    \n",
    "    '''print()\n",
    "\n",
    "    results = crossValidiation(XX, Y, k = 10, classifier = lr_clf, continious = False)\n",
    "    results['down'] = down\n",
    "    results['up'] = up\n",
    "    results['thres'] = threshold\n",
    "    print(\"accuracy = \",results['accuracy'])\n",
    "    print(\"AUC = \", results['auc'])\n",
    "    print(\"recal = \",results['recall'])\n",
    "    print(\"precision = \",results['precision'])\n",
    "    print(\"f1 = \",results['f1'])\n",
    "    results['len_snps'] = len(snpReduc[i])\n",
    "   # results['len snps'] = 500\n",
    "    writeResults[i] = results\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    writeResult(path,'llrcv'+i,writeResults,i)\n",
    "    writeResultConf(path,'llrcv'+i,writeResults2)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for iCat in categories:\n",
    "    \n",
    "    ids['coef']={}\n",
    "    XX = createNewTable(snpReduc[iCat],X)\n",
    "\n",
    "    lr_clf = linear_model.LogisticRegression()\n",
    "    lr_clf.fit(XX, Y)\n",
    "\n",
    "    coefs=lr_clf.coef_[0]\n",
    "    print(\"before = \",len(set(coefs)))\n",
    "    #for i in range(len(coefs)):\n",
    "     #   coefs[i] = abs(coefs[i])\n",
    "\n",
    "    print(\"after = \",len(set(coefs)))\n",
    "\n",
    "    idToName = {}\n",
    "    nameToId = {}\n",
    "\n",
    "    for i in range(len(coefs)):\n",
    "        nameToId[coefs[i]] = []\n",
    "\n",
    "    for i in range(len(coefs)):\n",
    "        nameToId[coefs[i]].append(i)\n",
    "        idToName[i] = coefs[i]\n",
    "\n",
    "\n",
    "    ids['coef']['nameToId'] = nameToId\n",
    "    ids['coef']['idToName'] = idToName\n",
    "\n",
    "\n",
    "    sc = sorted(coefs,reverse=True)\n",
    "\n",
    "    top_30 = []\n",
    "\n",
    "    for i in range(30):\n",
    "\n",
    "        snp = ids['coef']['nameToId'][sc[i]][0]\n",
    "        ids['coef']['nameToId'][sc[i]].remove(snp)\n",
    "        top_30.append(snpReduc[iCat][snp])\n",
    "     #   top_30.append(snp)\n",
    "\n",
    "    #snpReduc['low'] = top_30\n",
    "    writeCoef(path,top_30,sc,ids['snps']['idToName'], 'mycutoffabs')\n",
    "    #write.writeSnpsUsed(top_30,ids['snps']['idToName'],chromosomes,iCat+'_not_abs_top30')\n",
    "    #write.writeSnpsUsed(top_30,ids['snps']['idToName'],chromosomes,'top_30_all_abs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
